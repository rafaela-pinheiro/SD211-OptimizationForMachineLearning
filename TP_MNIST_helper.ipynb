{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149938de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ee878-16b7-46ab-811d-48aebc085cf1",
   "metadata": {},
   "source": [
    "# 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9afc7a6a-14b1-4bf8-8cbf-f15d61a2c1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "x_train = train_data[0].reshape(-1,784).astype(float) / 255\n",
    "y_train = train_data[1]\n",
    "\n",
    "x_test = test_data[0].reshape(-1,784).astype(float) / 255\n",
    "y_test = test_data[1]\n",
    "\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f055f2-a9f7-4905-bab0-d2d53287b65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c1fdb3ab50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbIElEQVR4nO3df2xV9f3H8dflRy+o7e1KaW/vKFgQYbGAGZOuQZmOBqgLA8EN0AkYAsEVM+ycrk7BX0knS5hxQfxngWkEGQk/lEQSKLSErdSAMEbmOtpUwdGWSdJ7oUjp6Of7B9n9eqWA53Jv3215PpKT0HvPp/e9szOeO723B59zzgkAgC7Wx3oAAMDNiQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT/awH+LqOjg6dOnVKqamp8vl81uMAADxyzuns2bMKhULq0+fq1zndLkCnTp1Sbm6u9RgAgBt08uRJDRky5KrPd7sfwaWmplqPAABIgOv9fZ60AK1Zs0a33367BgwYoIKCAn300UffaB0/dgOA3uF6f58nJUCbNm1SaWmpVq5cqY8//ljjxo3T1KlTdfr06WS8HACgJ3JJMGHCBFdSUhL9+tKlSy4UCrny8vLrrg2Hw04SGxsbG1sP38Lh8DX/vk/4FdDFixd16NAhFRUVRR/r06ePioqKVF1dfcX+bW1tikQiMRsAoPdLeIC++OILXbp0SdnZ2TGPZ2dnq6mp6Yr9y8vLFQgEohufgAOAm4P5p+DKysoUDoej28mTJ61HAgB0gYT/HlBmZqb69u2r5ubmmMebm5sVDAav2N/v98vv9yd6DABAN5fwK6CUlBSNHz9eFRUV0cc6OjpUUVGhwsLCRL8cAKCHSsqdEEpLS7VgwQJ973vf04QJE/T666+rtbVVjz/+eDJeDgDQAyUlQHPmzNF//vMfrVixQk1NTbr77ru1c+fOKz6YAAC4efmcc856iK+KRCIKBALWYwAAblA4HFZaWtpVnzf/FBwA4OZEgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMJDxAL774onw+X8w2evToRL8MAKCH65eMb3rXXXdp9+7d//8i/ZLyMgCAHiwpZejXr5+CwWAyvjUAoJdIyntAx48fVygU0vDhw/Xoo4/qxIkTV923ra1NkUgkZgMA9H4JD1BBQYHWr1+vnTt3au3atWpoaNB9992ns2fPdrp/eXm5AoFAdMvNzU30SACAbsjnnHPJfIGWlhYNGzZMq1ev1qJFi654vq2tTW1tbdGvI5EIEQKAXiAcDistLe2qzyf90wHp6em68847VVdX1+nzfr9ffr8/2WMAALqZpP8e0Llz51RfX6+cnJxkvxQAoAdJeICefvppVVVV6dNPP9Vf//pXPfTQQ+rbt6/mzZuX6JcCAPRgCf8R3Oeff6558+bpzJkzGjx4sO69914dOHBAgwcPTvRLAQB6sKR/CMGrSCSiQCBgPQbQ7YwdO9bzmieffDKu13r44Yc9r+mq/93u3bvX85rJkycnYRJcz/U+hMC94AAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE0n/B+mA3q64uNjzmrlz53pe89hjj3le05X3Gj558qTnNenp6Z7X3HfffZ7X3H333Z7XSNKRI0fiWodvhisgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOBu2OiVBg8eHNe6V155xfOaBQsWeF6TkpLiec2//vUvz2teffVVz2sk6e9//7vnNZ988onnNRs3bvS8Jp67j7e0tHheg+TjCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHNSNHt5ebmel6za9euuF5r5MiRntfEc6PLF154wfOaN9980/Oa8+fPe14Tr7KyMs9rfvzjH3te8/7773te8+mnn3peg+TjCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHNSNGlcnJyPK959913Pa+J56aiklRTU+N5zdKlSz2vOXr0qOc1XenXv/615zUvv/yy5zUffvih5zVz5871vAbdE1dAAAATBAgAYMJzgPbt26fp06crFArJ5/Np27ZtMc8757RixQrl5ORo4MCBKioq0vHjxxM1LwCgl/AcoNbWVo0bN05r1qzp9PlVq1bpjTfe0FtvvaWamhrdeuutmjp1qi5cuHDDwwIAeg/PH0IoLi5WcXFxp8855/T666/r+eef14wZMyRJb7/9trKzs7Vt2zbePAQARCX0PaCGhgY1NTWpqKgo+lggEFBBQYGqq6s7XdPW1qZIJBKzAQB6v4QGqKmpSZKUnZ0d83h2dnb0ua8rLy9XIBCIbrm5uYkcCQDQTZl/Cq6srEzhcDi6nTx50nokAEAXSGiAgsGgJKm5uTnm8ebm5uhzX+f3+5WWlhazAQB6v4QGKC8vT8FgUBUVFdHHIpGIampqVFhYmMiXAgD0cJ4/BXfu3DnV1dVFv25oaNCRI0eUkZGhoUOHavny5Xr11Vc1cuRI5eXl6YUXXlAoFNLMmTMTOTcAoIfzHKCDBw/qgQceiH5dWloqSVqwYIHWr1+vZ555Rq2trVqyZIlaWlp07733aufOnRowYEDipgYA9Hg+55yzHuKrIpGIAoGA9RhIknfeecfzmp/97Gee17z//vue10jS/PnzPa8Jh8NxvZZX6enpntdc7RfGr2fevHme12zfvt3zmp/+9Kee17S3t3teAxvhcPia7+ubfwoOAHBzIkAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAnuho243X333Z7X1NTUeF5z7tw5z2sefPBBz2uk+ObrKn/72988r8nPz0/CJJ0bPny45zWfffZZEiZBd8HdsAEA3RIBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYKKf9QDoudLT0z2v6dfP+ym3fft2z2u6801F4zVmzBjPa7rZvYaBGFwBAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmuBkpYGDdunXWI1zTpk2bPK/597//nYRJ0JtxBQQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOBmpOj2fvKTn3TZawUCAc9rHnjgAc9r0tLSPK/pSqtXr/a85r///W8SJkFvxhUQAMAEAQIAmPAcoH379mn69OkKhULy+Xzatm1bzPMLFy6Uz+eL2aZNm5aoeQEAvYTnALW2tmrcuHFas2bNVfeZNm2aGhsbo9vGjRtvaEgAQO/j+UMIxcXFKi4uvuY+fr9fwWAw7qEAAL1fUt4DqqysVFZWlkaNGqUnnnhCZ86cueq+bW1tikQiMRsAoPdLeICmTZumt99+WxUVFXrttddUVVWl4uJiXbp0qdP9y8vLFQgEoltubm6iRwIAdEMJ/z2guXPnRv88ZswYjR07ViNGjFBlZaUmT558xf5lZWUqLS2Nfh2JRIgQANwEkv4x7OHDhyszM1N1dXWdPu/3+5WWlhazAQB6v6QH6PPPP9eZM2eUk5OT7JcCAPQgnn8Ed+7cuZirmYaGBh05ckQZGRnKyMjQSy+9pNmzZysYDKq+vl7PPPOM7rjjDk2dOjWhgwMAejbPATp48GDMva/+9/7NggULtHbtWh09elR/+tOf1NLSolAopClTpuiVV16R3+9P3NQAgB7P55xz1kN8VSQSieuGkOgZrvULzFezePFiz2v69eu6++y2t7d7XtO/f3/Pa3w+n+c177zzjuc1kjR//vy41gFfFQ6Hr/m+PveCAwCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAnuho1uLz8/v0vWSFJjY6PnNY8//rjnNY899pjnNfH8T3XixIme10hSTU1NXOuAr+Ju2ACAbokAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMNHPegDgeo4dO9YlayRp1KhRntdMnz49rtfy6rXXXvO8hpuKojvjCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHNSIGvWLZsmec16enpiR+kEx9++GGXvA7QVbgCAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMcDNS9Er5+flxrZs7d26CJ+ncK6+84nnN/v37kzAJYIcrIACACQIEADDhKUDl5eW65557lJqaqqysLM2cOVO1tbUx+1y4cEElJSUaNGiQbrvtNs2ePVvNzc0JHRoA0PN5ClBVVZVKSkp04MAB7dq1S+3t7ZoyZYpaW1uj+zz11FP64IMPtHnzZlVVVenUqVOaNWtWwgcHAPRsnj6EsHPnzpiv169fr6ysLB06dEiTJk1SOBzWH//4R23YsEE//OEPJUnr1q3Td77zHR04cEDf//73Ezc5AKBHu6H3gMLhsCQpIyNDknTo0CG1t7erqKgous/o0aM1dOhQVVdXd/o92traFIlEYjYAQO8Xd4A6Ojq0fPlyTZw4MfqR16amJqWkpCg9PT1m3+zsbDU1NXX6fcrLyxUIBKJbbm5uvCMBAHqQuANUUlKiY8eO6b333ruhAcrKyhQOh6PbyZMnb+j7AQB6hrh+EXXZsmXasWOH9u3bpyFDhkQfDwaDunjxolpaWmKugpqbmxUMBjv9Xn6/X36/P54xAAA9mKcrIOecli1bpq1bt2rPnj3Ky8uLeX78+PHq37+/Kioqoo/V1tbqxIkTKiwsTMzEAIBewdMVUElJiTZs2KDt27crNTU1+r5OIBDQwIEDFQgEtGjRIpWWliojI0NpaWl68sknVVhYyCfgAAAxPAVo7dq1kqT7778/5vF169Zp4cKFkqTf//736tOnj2bPnq22tjZNnTpVb775ZkKGBQD0Hj7nnLMe4qsikYgCgYD1GOhGfD6f5zWbNm2K67Uefvhhz2sOHz7seU08PxFob2/3vAawFA6HlZaWdtXnuRccAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATMT1L6ICXelHP/qR5zWzZ8+O67Xa2to8r/nNb37jeQ13tga4AgIAGCFAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHAzUnR7ixYt6rLX2rJli+c1O3fuTMIkQO/HFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKbkaJLzZs3z/OaGTNmeF5TW1vreY0kPffcc3GtA+AdV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAluRoouNX/+/C55nXXr1sW17rPPPkvwJACuhisgAIAJAgQAMOEpQOXl5brnnnuUmpqqrKwszZw584p/d+X++++Xz+eL2ZYuXZrQoQEAPZ+nAFVVVamkpEQHDhzQrl271N7erilTpqi1tTVmv8WLF6uxsTG6rVq1KqFDAwB6Pk8fQti5c2fM1+vXr1dWVpYOHTqkSZMmRR+/5ZZbFAwGEzMhAKBXuqH3gMLhsCQpIyMj5vF3331XmZmZys/PV1lZmc6fP3/V79HW1qZIJBKzAQB6v7g/ht3R0aHly5dr4sSJys/Pjz7+yCOPaNiwYQqFQjp69KieffZZ1dbWasuWLZ1+n/Lycr300kvxjgEA6KHiDlBJSYmOHTum/fv3xzy+ZMmS6J/HjBmjnJwcTZ48WfX19RoxYsQV36esrEylpaXRryORiHJzc+MdCwDQQ8QVoGXLlmnHjh3at2+fhgwZcs19CwoKJEl1dXWdBsjv98vv98czBgCgB/MUIOecnnzySW3dulWVlZXKy8u77pojR45IknJycuIaEADQO3kKUElJiTZs2KDt27crNTVVTU1NkqRAIKCBAweqvr5eGzZs0IMPPqhBgwbp6NGjeuqppzRp0iSNHTs2Kf8BAAA9k6cArV27VtLlXzb9qnXr1mnhwoVKSUnR7t279frrr6u1tVW5ubmaPXu2nn/++YQNDADoHTz/CO5acnNzVVVVdUMDAQBuDj53vap0sUgkokAgYD0GAOAGhcNhpaWlXfV5bkYKADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiW4XIOec9QgAgAS43t/n3S5AZ8+etR4BAJAA1/v73Oe62SVHR0eHTp06pdTUVPl8vpjnIpGIcnNzdfLkSaWlpRlNaI/jcBnH4TKOw2Uch8u6w3Fwzuns2bMKhULq0+fq1zn9unCmb6RPnz4aMmTINfdJS0u7qU+w/+E4XMZxuIzjcBnH4TLr4xAIBK67T7f7ERwA4OZAgAAAJnpUgPx+v1auXCm/3289iimOw2Uch8s4DpdxHC7rSceh230IAQBwc+hRV0AAgN6DAAEATBAgAIAJAgQAMNFjArRmzRrdfvvtGjBggAoKCvTRRx9Zj9TlXnzxRfl8vpht9OjR1mMl3b59+zR9+nSFQiH5fD5t27Yt5nnnnFasWKGcnBwNHDhQRUVFOn78uM2wSXS947Bw4cIrzo9p06bZDJsk5eXluueee5SamqqsrCzNnDlTtbW1MftcuHBBJSUlGjRokG677TbNnj1bzc3NRhMnxzc5Dvfff/8V58PSpUuNJu5cjwjQpk2bVFpaqpUrV+rjjz/WuHHjNHXqVJ0+fdp6tC531113qbGxMbrt37/feqSka21t1bhx47RmzZpOn1+1apXeeOMNvfXWW6qpqdGtt96qqVOn6sKFC108aXJd7zhI0rRp02LOj40bN3bhhMlXVVWlkpISHThwQLt27VJ7e7umTJmi1tbW6D5PPfWUPvjgA23evFlVVVU6deqUZs2aZTh14n2T4yBJixcvjjkfVq1aZTTxVbgeYMKECa6kpCT69aVLl1woFHLl5eWGU3W9lStXunHjxlmPYUqS27p1a/Trjo4OFwwG3e9+97voYy0tLc7v97uNGzcaTNg1vn4cnHNuwYIFbsaMGSbzWDl9+rST5Kqqqpxzl/+779+/v9u8eXN0n08++cRJctXV1VZjJt3Xj4Nzzv3gBz9wv/jFL+yG+ga6/RXQxYsXdejQIRUVFUUf69Onj4qKilRdXW04mY3jx48rFApp+PDhevTRR3XixAnrkUw1NDSoqakp5vwIBAIqKCi4Kc+PyspKZWVladSoUXriiSd05swZ65GSKhwOS5IyMjIkSYcOHVJ7e3vM+TB69GgNHTq0V58PXz8O//Puu+8qMzNT+fn5Kisr0/nz5y3Gu6pudzPSr/viiy906dIlZWdnxzyenZ2tf/7zn0ZT2SgoKND69es1atQoNTY26qWXXtJ9992nY8eOKTU11Xo8E01NTZLU6fnxv+duFtOmTdOsWbOUl5en+vp6PffccyouLlZ1dbX69u1rPV7CdXR0aPny5Zo4caLy8/MlXT4fUlJSlJ6eHrNvbz4fOjsOkvTII49o2LBhCoVCOnr0qJ599lnV1tZqy5YthtPG6vYBwv8rLi6O/nns2LEqKCjQsGHD9Oc//1mLFi0ynAzdwdy5c6N/HjNmjMaOHasRI0aosrJSkydPNpwsOUpKSnTs2LGb4n3Qa7nacViyZEn0z2PGjFFOTo4mT56s+vp6jRgxoqvH7FS3/xFcZmam+vbte8WnWJqbmxUMBo2m6h7S09N15513qq6uznoUM/87Bzg/rjR8+HBlZmb2yvNj2bJl2rFjh/bu3Rvzz7cEg0FdvHhRLS0tMfv31vPhasehMwUFBZLUrc6Hbh+glJQUjR8/XhUVFdHHOjo6VFFRocLCQsPJ7J07d0719fXKycmxHsVMXl6egsFgzPkRiURUU1Nz058fn3/+uc6cOdOrzg/nnJYtW6atW7dqz549ysvLi3l+/Pjx6t+/f8z5UFtbqxMnTvSq8+F6x6EzR44ckaTudT5Yfwrim3jvvfec3+9369evd//4xz/ckiVLXHp6umtqarIerUv98pe/dJWVla6hocH95S9/cUVFRS4zM9OdPn3aerSkOnv2rDt8+LA7fPiwk+RWr17tDh8+7D777DPnnHO//e1vXXp6utu+fbs7evSomzFjhsvLy3Nffvml8eSJda3jcPbsWff000+76upq19DQ4Hbv3u2++93vupEjR7oLFy5Yj54wTzzxhAsEAq6ystI1NjZGt/Pnz0f3Wbp0qRs6dKjbs2ePO3jwoCssLHSFhYWGUyfe9Y5DXV2de/nll93BgwddQ0OD2759uxs+fLibNGmS8eSxekSAnHPuD3/4gxs6dKhLSUlxEyZMcAcOHLAeqcvNmTPH5eTkuJSUFPftb3/bzZkzx9XV1VmPlXR79+51kq7YFixY4Jy7/FHsF154wWVnZzu/3+8mT57samtrbYdOgmsdh/Pnz7spU6a4wYMHu/79+7thw4a5xYsX97r/k9bZf35Jbt26ddF9vvzyS/fzn//cfetb33K33HKLe+ihh1xjY6Pd0ElwveNw4sQJN2nSJJeRkeH8fr+744473K9+9SsXDodtB/8a/jkGAICJbv8eEACgdyJAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPwffImsfZ9N2rwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exploring the data\n",
    "i = np.random.randint(y_train.size)\n",
    "\n",
    "print(\"label is\", y_train[i])\n",
    "plt.imshow(train_data[0][i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feee3fb-8b54-4cd2-8ee1-a003d1292f8a",
   "metadata": {},
   "source": [
    "# 2. Building the model\n",
    "\n",
    "We use Keras to build a fully-connected neural network.\n",
    "\n",
    "For more details, see https://keras.io/guides/training_with_built_in_methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b521b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rafin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " digits (InputLayer)         [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26506 (103.54 KB)\n",
      "Trainable params: 26506 (103.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# source : https://keras.io/guides/training_with_built_in_methods/\n",
    "\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(32, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.Dense(32, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "loss = keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b18b6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 6, 6, 6, 9, 2, 4, 3, 9, 4, 6, 9, 0, 9, 9, 9, 0, 3, 4, 9, 6,\n",
       "       9, 6, 9, 9, 9, 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# untrained model\n",
    "predicted_probas = model(x_train)\n",
    "y_pred = np.argmax(predicted_probas, 1) # the prediction is the class with highest probability\n",
    "y_pred[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d74f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0,\n",
       "       9, 1, 1, 2, 4, 3, 2, 7], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what we would like to get\n",
    "y_train[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c1255b-36d0-4d05-a5a0-fa9408742b56",
   "metadata": {},
   "source": [
    "# 3. Computing gradients\n",
    "\n",
    "We are now going to implement the stochastic gradient method using the Keras functionalities. \n",
    "\n",
    "First, we need to understand how to compute gradients using auto-differentation (also called **backpropagation**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7cbc050-6044-4149-9354-0d3870837d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 784\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Source: https://keras.io/guides/writing_a_custom_training_loop_in_tensorflow/\n",
    "\n",
    "# Example of gradient computation using automatic differentiation\n",
    "\n",
    "# selecting a batch\n",
    "x_train_batch = x_train[0:1]\n",
    "y_train_batch = y_train[0:1]\n",
    "\n",
    "# Open a GradientTape to record the operations run\n",
    "# during the forward pass, which enables auto-differentiation.\n",
    "with tf.GradientTape() as tape:\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "    \n",
    "        logits = model(x_train_batch)  # Logits for this minibatch (the logits are the name we give to the Neural Network output)\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value = loss(y_train_batch, logits)\n",
    "\n",
    "# Use the gradient tape to automatically retrieve\n",
    "# the gradients of the trainable variables with respect to the loss.\n",
    "grad = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "print(len(grad), len(grad[0]))\n",
    "print(len(x_train_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f189a1-5cec-4723-93fc-8693ec8d6a5c",
   "metadata": {},
   "source": [
    "We also define two important helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab2abd2-91e5-4b9e-a863-e1e3ea3dcd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## update the weights by adding real_factor * update\n",
    "def update_weights(model, update, real_factor = 1.):\n",
    "    new_weights = model.trainable_weights.copy()\n",
    "\n",
    "    # iterate over all layers\n",
    "    for i in range(len(new_weights)):\n",
    "        new_weights[i] = new_weights[i] + real_factor * update[i]\n",
    "        \n",
    "    model.set_weights(new_weights)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "def mean_loss(model, X, Y):\n",
    "    logits = model(X)\n",
    "    return np.mean(loss(Y, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c11dcf5-bcd8-403b-ae60-41c755b5c5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3214273"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the train loss on the whole dataset\n",
    "# this is a costly operation, use sparsely\n",
    "mean_loss(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0785ce-c5e2-4103-b4a8-1b10df2abf4d",
   "metadata": {},
   "source": [
    "# 4. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e73474",
   "metadata": {},
   "source": [
    "## 4.1\n",
    "In total, we have 26,506 optimization variables (or parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b80e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções auxiliares para a NN\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n",
    "    \n",
    "def softmax(z):\n",
    "    return np.exp(z) / sum(np.exp(z))\n",
    "\n",
    "def relu_deriv(z):\n",
    "    return z > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "246c8ebb-a57e-4548-a69c-338055cf04a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient algorithm\n",
    "# for k = 0,1,...,k\n",
    "#   choose data sample i_k\n",
    "#   compute f_{ik}(O_k)\n",
    "# compute g_h = grad f_ik(O_k)]\n",
    "# update O_{k+1} = O_k - gamma_k g_h\n",
    "\n",
    "# Função de backpropagation. Recebe o x e y lidos atualmente no batch para a realização dos cálculos\n",
    "\n",
    "\n",
    "\n",
    "def backprop(self, x, y):\n",
    "    # derivatives of the weights and biases\n",
    "    der_b = [0, 0]\n",
    "    der_w = [0, 0]\n",
    "    \n",
    "    activations, z = self.forward(x)\n",
    "\n",
    "    y = self.y_vec(y)\n",
    "    \n",
    "    # partial derivatives of the output layer    \n",
    "    delta = (activations[-1] - y) \n",
    "    der_b[-1] = (activations[-1] - y)\n",
    "    der_w[-1] = np.dot(delta, activations[-2].T)\n",
    "    \n",
    "    # partial derivatives of the hidden layer\n",
    "    delta = relu_deriv(z[-2]) * (np.dot(self.weights[-1].T, (activations[-1] - y))) \n",
    "    der_b[-2] = delta \n",
    "    der_w[-2] = np.dot(delta, x)\n",
    "\n",
    "    return der_b, der_w\n",
    "\n",
    "# TO IMPLEMENT\n",
    "def SGD(model, x_train, y_train, learning_rate, epochs=1, batch_size=1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79465229-cdc5-47d2-9a01-ad10f01132b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = build_model()\n",
    "\n",
    "# run SGD \n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d9bf5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    # initiate network with number of layers and their dimensions in the dim attribute\n",
    "    def __init__(self, model, loss, epochs, learning_rate, batch_size=1, mode =\"sgd\"):\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "\n",
    "    def update_weights(self, gradient, real_factor = 1.):\n",
    "        new_weights = model.trainable_weights.copy()\n",
    "\n",
    "        # iterate over all layers\n",
    "        for i in range(len(new_weights)):\n",
    "            new_weights[i] = new_weights[i] + real_factor  * gradient[i]\n",
    "        self.model.set_weights(new_weights)\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        if self.mode == \"sgd\":\n",
    "            # Extrai um único exemplo e adiciona uma dimensão de lote\n",
    "            x_train_batch = tf.expand_dims(batch[0][0], axis=0)  # Formato (1, 784)\n",
    "            y_train_batch = tf.expand_dims(batch[0][1], axis=0)  # Formato (1,)\n",
    "        else:\n",
    "            # Já é um lote, mas garante que está no formato correto\n",
    "            x_train_batch = tf.stack([sample[0] for sample in batch])  # Formato (batch_size, 784)\n",
    "            y_train_batch = tf.stack([sample[1] for sample in batch])  # Formato (batch_size,)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x_train_batch, training=True)  # Logits para este minibatch\n",
    "            loss_value = self.loss(y_train_batch, logits)\n",
    "\n",
    "        return loss_value, tape\n",
    "    \n",
    "    def backpropagation(self, loss_value, tape):\n",
    "        return tape.gradient(loss_value, self.model.trainable_weights)\n",
    " \n",
    "    def make_batch(self, data):\n",
    "        if (self.mode == \"sgd\"):\n",
    "            idx_sample = random.randint(0, len(data[0]))\n",
    "            result = ([( data[0][idx_sample], data[1][idx_sample])])\n",
    "            print(\"ALOOOOOO\", idx_sample)\n",
    "            return result\n",
    "        \n",
    "        return [ (data[0][k:k+self.batch_size], data[1][k:k+self.batch_size])\n",
    "                  for k in range(0, len(data[0]), self.batch_size)]\n",
    "    \n",
    "    # Função de treinamento da rede neural\n",
    "    def training(self, data, test_data):\n",
    "        \n",
    "        for ep in range(self.epochs):\n",
    "            random.shuffle(list(data))\n",
    "            batch = self.make_batch(data)\n",
    "\n",
    "            loss, tape = self.forward(batch)\n",
    "\n",
    "            grad = self.backpropagation(loss, tape)\n",
    "\n",
    "            if (self.mode == \"sgd\"):\n",
    "                self.update_weights(grad, real_factor= (-1) * self.learning_rate)\n",
    "            else:\n",
    "                for e, batch in enumerate(batch):\n",
    "                    self.update_weights(grad, real_factor=self.learning_rate)\n",
    "\n",
    "            self.check(test_data, ep)\n",
    "           \n",
    "    def check(self, test_data, ep):\n",
    "        images, labels = test_data\n",
    "        acertos = 0\n",
    "        for x, y in zip(images, labels):\n",
    "            x = tf.expand_dims(x, axis=0)  # Adiciona uma dimensão de lote\n",
    "            logits = self.model(x, training=False)  # Passa pelo modelo\n",
    "            if np.argmax(logits) == y:\n",
    "                acertos += 1\n",
    "\n",
    "        print(f\"Época {ep + 1}, Acertos: {acertos}/{len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24d39aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALOOOOOO 15402\n",
      "Época 1, Acertos: 1288/10000\n",
      "ALOOOOOO 26790\n",
      "Época 2, Acertos: 831/10000\n",
      "ALOOOOOO 41631\n",
      "Época 3, Acertos: 926/10000\n",
      "ALOOOOOO 42287\n",
      "Época 4, Acertos: 1044/10000\n",
      "ALOOOOOO 47824\n",
      "Época 5, Acertos: 1074/10000\n",
      "ALOOOOOO 30106\n",
      "Época 6, Acertos: 1032/10000\n",
      "ALOOOOOO 43953\n",
      "Época 7, Acertos: 1054/10000\n",
      "ALOOOOOO 27374\n",
      "Época 8, Acertos: 1149/10000\n",
      "ALOOOOOO 19405\n",
      "Época 9, Acertos: 733/10000\n",
      "ALOOOOOO 57983\n",
      "Época 10, Acertos: 996/10000\n"
     ]
    }
   ],
   "source": [
    "net = Neural_Network(model = model, loss=loss, epochs = 10, learning_rate = 0.5)\n",
    "net.training((x_train, y_train), (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1\n",
    "The stochastic gradient has very noisy updates, while using the mini-batch gradient can decrease the noise due to its bigger batch size. This causes the mini-batch model to have a faster and more stable convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 1\n",
    "Evaluate the accuracy of the models on the training set and on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
